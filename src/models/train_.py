import numpy as np
import pandas as pd
from lifelines import NelsonAalenFitter
import warnings
import pandas as pd
from sksurv.ensemble import RandomSurvivalForest
from sksurv.util import Surv
from collections import Counter
from sklearn.utils import resample
import shap
from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance

warnings.filterwarnings("ignore", category=UserWarning)

DATA_PATH = "data/processed/lendingclub_features_for_rf.parquet"
FEATURE_PATH = "data/processed/features_final_list_rf.csv"

# --------------------------------------------------
# 1) ë°ì´í„° ë¡œë“œ & í•„í„°
# --------------------------------------------------
df = pd.read_parquet(DATA_PATH)
df["issue_d"] = pd.to_datetime(df["issue_d"], errors="coerce")
df["last_pymnt_d"] = pd.to_datetime(df["last_pymnt_d"], errors="coerce")

# â–¼â–¼â–¼ ì¶”ê°€ ì½”ë“œ â–¼â–¼â–¼  
print(f"issue_d NaT ê°œìˆ˜: {df['issue_d'].isna().sum()}")
print(f"last_pymnt_d NaT ê°œìˆ˜: {df['last_pymnt_d'].isna().sum()}")
df = df[df['issue_d'].notna() & df['last_pymnt_d'].notna()]  # NaT ì œê±°


# --------------------------------------------------
# COVIDâ€‘19 ë…¸ì¶œ ë³€ìˆ˜ ì •ì˜
#   - cutâ€‘off ë‚ ì§œ : 2020â€‘03â€‘01
#   - ë¹„â€‘ë¶€ë„(Charged Off/Default ì•„ë‹˜) ëŒ€ì¶œ  â†’  issue_d + term â‰¤ cutâ€‘off
#   - ë¶€ë„ ëŒ€ì¶œ                              â†’  last_pymnt_d â‰¤ cutâ€‘off
# --------------------------------------------------
cutoff = pd.to_datetime("2020-03-01")

# termì´ '36 months' ê°™ì€ ë¬¸ìì—´ì´ë©´ ìˆ«ì(ì›”ìˆ˜)ë§Œ ì¶”ì¶œ
df["term"] = (
    df["term"].astype(str)
      .str.extract(r"(\d+)")[0]
      .astype(int)
)

# ë§Œê¸°ì¼ = issue_d + term_m Ã— 30ì¼ (ì›” ë‹¨ìœ„ ê·¼ì‚¬)
df["maturity_d"] = df["issue_d"] + pd.to_timedelta(df["term"] * 30, unit="D")

# ë¶€ë„(event==1)  â†’  ë§ˆì§€ë§‰ ìƒí™˜ì¼ì´ cutoff **ì´ì „(í¬í•¨)** ì´ë©´ ì½”ë¡œë‚˜ ë…¸ì¶œ
# ê·¸ ì™¸            â†’  ë§Œê¸°ì¼ì´ cutoff **ì´ì „(í¬í•¨)** ì´ë©´ ì½”ë¡œë‚˜ ë…¸ì¶œ
df["covid_exposure"] = np.where(
    df["E"] == 1,
    df["last_pymnt_d"] >= cutoff,
    df["maturity_d"]   >= cutoff
).astype(int)

# Calculate survival duration:
# If event occurred (event == 1), use duration until event (e.g. charged off)
# Else, use duration until end of observation (censored)
# T / event ê²°ì¸¡ & ìŒìˆ˜ ì œê±°
df = df.dropna(subset=["T", "E"])
df = df[df["T"] >= 0]

# --------------------------------------------------
# 2) íŠ¹ì§• í–‰ë ¬ / íƒ€ê¹ƒ
# --------------------------------------------------
features = pd.read_csv(FEATURE_PATH)["feature"].tolist()
features = [f for f in features if f in df.columns]
features = [f for f in features if f not in ["T", "E"] and str(df[f].dtype) not in ["object", "datetime64[ns]"]]

X = df[features]


# â–¼â–¼â–¼ ì¶”ê°€ ì½”ë“œ â–¼â–¼â–¼  
print("â–¶ ë°ì´í„° ë¶„í¬ í™•ì¸:")
print(f"- ì „ì²´ ë°ì´í„°: {len(df)}")
print(f"- COVID ë…¸ì¶œ ê·¸ë£¹: {df['covid_exposure'].sum()}")
print(f"- issue_d < cutoff: {(df['issue_d'] < cutoff).sum()}")
print(f"- ìµœì†Œ issue_d: {df['issue_d'].min()}, ìµœëŒ€ issue_d: {df['issue_d'].max()}")

df = df[df['covid_exposure'] == 1]
X = df[features].copy()

# --------------------------------------------------
# 3) ì‹œê°„ìˆœ ë¶„í• 
# --------------------------------------------------
# í›ˆë ¨ ë°ì´í„° ì¸ë±ìŠ¤ ë§ˆìŠ¤í¬: issue_d < cutoff & covid_exposure == 0
train_mask = (df['issue_d'] < cutoff).reindex(X.index).fillna(False)

# Debug: inspect mask components
print(f"â–¶ Total rows: {len(df)}")
print(f"â–¶ Rows with issue_d < cutoff ({cutoff.date()}): {int((df['issue_d'] < cutoff).sum())}")

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test = X.loc[train_mask], X.loc[~train_mask]
y_train = df.loc[train_mask, ["T", "E"]]
y_test  = df.loc[~train_mask, ["T", "E"]]
print(X_train.dtypes.value_counts()) # category íƒ€ì…ê³¼ ìˆ«ìí˜•(int/float)ë§Œ ì¡´ì¬í•´ì•¼ í•¨


# Fallback if no training data with covid_exposure == 0
if X_train.shape[0] == 0:
    print("âš ï¸ No non-exposed training data; using time-based split only.")
    train_mask = df["issue_d"] < cutoff
    X_train, X_test = X.loc[train_mask], X.loc[~train_mask]
    y_train = df.loc[train_mask, ["T", "E"]]
    y_test  = df.loc[~train_mask, ["T", "E"]]

# --------------------------------------------------
# 4) Nelson-Aalen ê¸°ë°˜ Hazard Rate ì¶”ì •
# --------------------------------------------------
naf = NelsonAalenFitter()
df["issue_month"] = df["issue_d"].dt.to_period("M")
monthly_hazards = {}

for month, group_df in df.groupby("issue_month"):
    if len(group_df) < 100:
        continue
    try:
        naf.fit(group_df["T"], event_observed=group_df["E"])
        cum_hazard = naf.cumulative_hazard_
        # ìˆœê°„ hazard â‰ˆ ëˆ„ì  hazard ì°¨ë¶„
        inst_hazard = cum_hazard.diff().fillna(0)
        monthly_hazards[str(month)] = inst_hazard.mean().values[0]
    except Exception as e:
        print(f"âš ï¸ {month} ì›” hazard ê³„ì‚° ì˜¤ë¥˜: {e}")
        continue

# hazard í‰ê· ê°’ ì‹œê³„ì—´ ì‹œê°í™”
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.plot(monthly_hazards.keys(), monthly_hazards.values(), marker="o", label="NA estimated hazard")
plt.title("Monthly Nelson-Aalen estimated Hazard Rate")
plt.xlabel("issued month")
plt.ylabel("avg Hazard")
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
print("âœ… NA ê¸°ë°˜ ì›”ë³„ hazard ì¶”ì • ì™„ë£Œ")

# --------------------------------------------------
# 5) Random Survival Forest ëª¨ë¸ í•™ìŠµ & í‰ê°€ (covid_exposure ê·¸ë£¹ë³„)
# --------------------------------------------------
print("â–¶ í›ˆë ¨ ê°€ëŠ¥ ë°ì´í„° í˜„í™©:")
print(f"- ì´ í–‰ ìˆ˜: {len(df)}")
print(f"- issue_d â‰¥ 2019-05-01: {len(df[df['issue_d'] >= '2015-01-01'])}")
print("\nğŸ” ì „ì²´ ë°ì´í„° ê¸°ë°˜ SHAP ë¶„ì„")
print(f"X shape: {X.shape}, df[['T', 'E']] shape: {df[['T', 'E']].shape}")

shap_runs = []
top_idx_list = []

for i in range(10):  # bootstrap iterations
    X_bs, y_bs = resample(X, df[["T", "E"]], replace=True, n_samples = 10000, random_state=42 + i)
    y_surv_bs = Surv.from_arrays(event=y_bs["E"].astype(bool).values, time=y_bs["T"].values)
    rsf = RandomSurvivalForest(
        n_estimators=50,
        min_samples_split=20,
        min_samples_leaf=30,
        max_features="sqrt",
        n_jobs=-1,
        random_state=42 + i
    )
    rsf.fit(X_bs.values, y_surv_bs)
    print(f"âœ… ë¶€íŠ¸ìŠ¤íŠ¸ë© {i+1}/10 í•™ìŠµ ì™„ë£Œ")

    result = permutation_importance(
        rsf,
        X_bs.values,
        y_surv_bs,
        n_repeats=10,
        random_state=42 + i,
        n_jobs=-1
    )
    shap_runs.append(result.importances_mean)
    top_idx = np.argsort(result.importances_mean)[::-1][:5]
    top_idx_list.extend(top_idx)

top_5_idx = [item[0] for item in Counter(top_idx_list).most_common(5)]
top_5_features = [features[i] for i in top_5_idx]
print(f"ğŸ” ì „ì²´ ê¸°ì¤€ Top 5 features:", top_5_features)

y_final = Surv.from_arrays(event=df["E"].astype(bool).values, time=df["T"].values)
rsf_shap = RandomSurvivalForest(
    n_estimators=30,
    min_samples_split=20,
    min_samples_leaf=30,
    max_features="sqrt",
    n_jobs=-1,
    random_state=123
)
rsf_shap.fit(X[top_5_features].values, y_final)
print("âœ… ìµœì¢… SHAPìš© RSF í•™ìŠµ ì™„ë£Œ")


# Restrict X to top 5 features for SHAP computation
X_top5 = X[top_5_features].copy()
X_top5["T"] = df["T"]
X_top5["E"] = df["E"]

# Stratified downsampling to 20,000 rows maintaining event proportion
X_top5_sampled, _ = train_test_split(
    X_top5,
    train_size=10000,
    stratify=X_top5["E"],
    random_state=999
)

y_top5_sampled = Surv.from_arrays(
    event=X_top5_sampled["E"].astype(bool),
    time=X_top5_sampled["T"]
)

X_top5_sampled = X_top5_sampled.drop(columns=["T", "E"])

explainer = shap.Explainer(rsf_shap.predict, X_top5_sampled, algorithm="permutation", max_evals=500)
shap_values = explainer(X_top5_sampled)
print("âœ… SHAP ê³„ì‚° ì™„ë£Œ (ìƒ˜í”Œ 20,000ê±´ ê¸°ì¤€)")
shap_exp = shap.Explanation(
    values=shap_values.values if isinstance(shap_values, shap.Explanation) else shap_values,
    data=X_top5_sampled.values,
    feature_names=top_5_features
)
# SHAP ê°’ êµ¬ì¡° í™•ì¸ ë° ë³€í™˜
shap_data = shap_exp.values
if shap_data.ndim != 2:
    shap_data = shap_data.reshape(-1, len(top_5_features))
# ì°¨ì› ê²€ì¦
assert shap_data.shape[1] == len(top_5_features), \
    f"ì°¨ì› ë¶ˆì¼ì¹˜: {shap_data.shape[1]} != {len(top_5_features)}"

# 4. DataFrame ìƒì„±
shap_df = pd.DataFrame(
    data=shap_data,
    columns=top_5_features,
    index=X_top5_sampled.index
).join(df[['issue_month']], how='left')

# 5. ì›”ë³„ í‰ê·  ê³„ì‚°
mean_by_month = shap_df.groupby("issue_month")[top_5_features].mean()
print("âœ… ì›”ë³„ SHAP í‰ê· ê°’ ê³„ì‚° ì™„ë£Œ")
top_features_by_month = {"all": mean_by_month}

# ì €ì¥ ë˜ëŠ” ì‹œê°í™”ìš© ê²°ê³¼ ì¤€ë¹„ë¨

# --------------------------------------------------
# 6) í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
# --------------------------------------------------
import matplotlib.pyplot as plt
import seaborn as sns

# ì‹œê°í™”: ì›”ë³„ top featureë“¤ì˜ SHAP í‰ê· ê°’ ì¶”ì´
monthly_df = top_features_by_month["all"]
plt.figure(figsize=(12, 6))
for col in monthly_df.columns:
    sns.lineplot(data=monthly_df[col], label=col)
plt.title(f"Monthly SHAP average")
plt.xlabel("issue_month")
plt.ylabel("Average SHAP value")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ì‹œê°í™”: ê° ê·¸ë£¹ì—ì„œ top featureë“¤ì˜ SHAP ì´í•© ë¹„ì¤‘ (ë¹„ìœ¨ ê¸°ë°˜ ì¤‘ìš”ë„)
mean_abs = monthly_df.abs().sum()
mean_abs = mean_abs / mean_abs.sum()  # Normalize to sum=1
plt.figure(figsize=(6, 4))
sns.barplot(x=mean_abs.values, y=mean_abs.index)
plt.title(f"Top 5 features SHAP importance")
plt.xlabel("SHAP importance")
plt.ylabel("feature")
plt.tight_layout()
plt.show()