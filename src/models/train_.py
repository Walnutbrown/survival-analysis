import numpy as np
import pandas as pd
from lifelines import NelsonAalenFitter
import warnings
import shap

warnings.filterwarnings("ignore", category=UserWarning)

DATA_PATH = "../../data/processed/lendingclub_features_for_rf.parquet"
FEATURE_PATH = "../../data/processed/features_final_list_rf.csv"

# --------------------------------------------------
# 1) ë°ì´í„° ë¡œë“œ & í•„í„°
# --------------------------------------------------
df = pd.read_parquet(DATA_PATH)
df["issue_d"] = pd.to_datetime(df["issue_d"], errors="coerce")
df["last_pymnt_d"] = pd.to_datetime(df["last_pymnt_d"], errors="coerce")

# ê²°ì¸¡í•­ ì œê±°  
print(f"issue_d NaT ê°œìˆ˜: {df['issue_d'].isna().sum()}")
print(f"last_pymnt_d NaT ê°œìˆ˜: {df['last_pymnt_d'].isna().sum()}")
df = df[df['issue_d'].notna() & df['last_pymnt_d'].notna()]  # NaT ì œê±°


# --------------------------------------------------
# COVIDâ€‘19 ë…¸ì¶œ ë³€ìˆ˜ ì •ì˜
#   - cutâ€‘off ë‚ ì§œ : 2020â€‘03â€‘01
#   - ë¹„â€‘ë¶€ë„(Charged Off/Default ì•„ë‹˜) ëŒ€ì¶œ  â†’  issue_d + term â‰¤ cutâ€‘off
#   - ë¶€ë„ ëŒ€ì¶œ                              â†’  last_pymnt_d â‰¤ cutâ€‘off
# --------------------------------------------------
cutoff = pd.to_datetime("2020-03-01")

# termì´ '36 months' ê°™ì€ ë¬¸ìì—´ì´ë©´ ìˆ«ì(ì›”ìˆ˜)ë§Œ ì¶”ì¶œ
df["term"] = (
    df["term"].astype(str)
      .str.extract(r"(\d+)")[0]
      .astype(int)
)

# ë§Œê¸°ì¼ = issue_d + term_m Ã— 30ì¼ (ì›” ë‹¨ìœ„ ê·¼ì‚¬)
df["maturity_d"] = df["issue_d"] + pd.to_timedelta(df["term"] * 30, unit="D")

# ë¶€ë„(event==1)  â†’  ë§ˆì§€ë§‰ ìƒí™˜ì¼ì´ cutoff **ì´ì „(í¬í•¨)** ì´ë©´ ì½”ë¡œë‚˜ ë…¸ì¶œ
# ê·¸ ì™¸            â†’  ë§Œê¸°ì¼ì´ cutoff **ì´ì „(í¬í•¨)** ì´ë©´ ì½”ë¡œë‚˜ ë…¸ì¶œ
df["covid_exposure"] = np.where(
    df["E"] == 1,
    df["last_pymnt_d"] >= cutoff,
    df["maturity_d"]   >= cutoff
).astype(int)

# Calculate survival duration:
# If event occurred (event == 1), use duration until event (e.g. charged off)
# Else, use duration until end of observation (censored)
# T / event ê²°ì¸¡ & ìŒìˆ˜ ì œê±°
df = df.dropna(subset=["T", "E"])
df = df[df["T"] >= 0]

# --------------------------------------------------
# 2) íŠ¹ì§• í–‰ë ¬ / íƒ€ê¹ƒ
# --------------------------------------------------
features = pd.read_csv(FEATURE_PATH)["feature"].tolist()
features = [f for f in features if f in df.columns]
features = [f for f in features if f not in ["T", "E"] and str(df[f].dtype) not in ["object", "datetime64[ns]"]]

X = df[features]


# â–¼â–¼â–¼ ì¶”ê°€ ì½”ë“œ â–¼â–¼â–¼  
print("â–¶ ë°ì´í„° ë¶„í¬ í™•ì¸:")
print(f"- ì „ì²´ ë°ì´í„°: {len(df)}")
print(f"- COVID ë…¸ì¶œ ê·¸ë£¹: {df['covid_exposure'].sum()}")
print(f"- issue_d < cutoff: {(df['issue_d'] < cutoff).sum()}")
print(f"- ìµœì†Œ issue_d: {df['issue_d'].min()}, ìµœëŒ€ issue_d: {df['issue_d'].max()}")

df = df[df['covid_exposure'] == 1]
X = df[features].copy()

# --------------------------------------------------
# 3) ì‹œê°„ìˆœ ë¶„í• 
# --------------------------------------------------
# í›ˆë ¨ ë°ì´í„° ì¸ë±ìŠ¤ ë§ˆìŠ¤í¬: issue_d < cutoff & covid_exposure == 0
train_mask = (df['issue_d'] < cutoff).reindex(X.index).fillna(False)

# Debug: inspect mask components
print(f"â–¶ Total rows: {len(df)}")
print(f"â–¶ Rows with issue_d < cutoff ({cutoff.date()}): {int((df['issue_d'] < cutoff).sum())}")

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test = X.loc[train_mask], X.loc[~train_mask]
y_train = df.loc[train_mask, ["T", "E"]]
y_test  = df.loc[~train_mask, ["T", "E"]]
print(X_train.dtypes.value_counts()) # category íƒ€ì…ê³¼ ìˆ«ìí˜•(int/float)ë§Œ ì¡´ì¬í•´ì•¼ í•¨


# Fallback if no training data with covid_exposure == 0
if X_train.shape[0] == 0:
    print("âš ï¸ No non-exposed training data; using time-based split only.")
    train_mask = df["issue_d"] < cutoff
    X_train, X_test = X.loc[train_mask], X.loc[~train_mask]
    y_train = df.loc[train_mask, ["T", "E"]]
    y_test  = df.loc[~train_mask, ["T", "E"]]

# --------------------------------------------------
# 4) Nelson-Aalen ê¸°ë°˜ Hazard Rate ì¶”ì •
# --------------------------------------------------
naf = NelsonAalenFitter()
df["issue_month"] = df["issue_d"].dt.to_period("M")
monthly_hazards = {}
monthly_events = {}
monthly_ci = {}

for month, group_df in df.groupby("issue_month"):
    if len(group_df) < 100:
        continue
    try:
        naf.fit(group_df["T"], event_observed=group_df["E"])
        cum_hazard = naf.cumulative_hazard_
        inst_hazard = cum_hazard.diff().fillna(0)
        monthly_hazards[str(month)] = inst_hazard.mean().values[0]

        # ì¶”ê°€ ê¸°ëŠ¥ 1: ì›”ë³„ ë¶€ë„ ì‚¬ê±´ ìˆ˜ ì €ì¥
        monthly_events[str(month)] = int(group_df["E"].sum())

        # ì¶”ê°€ ê¸°ëŠ¥ 2: ì‹ ë¢°êµ¬ê°„ ì €ì¥ (ë§ˆì§€ë§‰ ì‹œì  ê¸°ì¤€)
        ci_df = naf.confidence_interval_
        if not ci_df.empty:
            last_ci = ci_df.iloc[-1]
            monthly_ci[str(month)] = (last_ci[0], last_ci[1])
    except Exception as e:
        print(f"âš ï¸ {month} ì›” hazard ê³„ì‚° ì˜¤ë¥˜: {e}")
        continue

# hazard í‰ê· ê°’ ì‹œê³„ì—´ ì‹œê°í™”
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.plot(monthly_hazards.keys(), monthly_hazards.values(), marker="o", label="NA estimated hazard")
plt.title("Monthly Nelson-Aalen estimated Hazard Rate")
plt.xlabel("issued month")
plt.ylabel("avg Hazard")
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
print("âœ… NA ê¸°ë°˜ ì›”ë³„ hazard ì¶”ì • ì™„ë£Œ")

# --------------------------------------------------
# 5) XGBoost AFT ëª¨ë¸ í•™ìŠµ & SHAP ë¶„ì„
# --------------------------------------------------
import xgboost as xgb


# AFTìš© label êµ¬ì„±
df_model = df.loc[X.index]
y_lower = np.where(df_model["E"] == 1, df_model["T"], -np.inf)
y_upper = df_model["T"]

# DMatrix êµ¬ì„±
dtrain = xgb.DMatrix(data=X, label=y_upper)
dtrain.set_float_info("label_lower_bound", y_lower)
dtrain.set_float_info("label_upper_bound", y_upper)

params = {
    "objective": "survival:aft",
    "aft_loss_distribution": "normal",
    "aft_loss_distribution_scale": 1.0,
    "learning_rate": 0.05,
    "max_depth": 4,
    "subsample": 0.8,
    "colsample_bynode": 0.8,
    "random_state": 42,
    "nthread": -1,
    "verbosity": 1
}

model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=100
)

# SHAP ê³„ì‚°
explainer = shap.TreeExplainer(model, data=X, feature_perturbation="interventional", approximate=True)
shap_values = explainer.shap_values(X)

# ì›”ë³„ í‰ê·  SHAP ê³„ì‚°
X["issue_month"] = df["issue_d"].dt.to_period("M")
shap_df = pd.DataFrame(shap_values, columns=features)
shap_df["issue_month"] = X["issue_month"]
monthly_shap = shap_df.groupby("issue_month")[features].mean()
print("âœ… XGBoost AFT ê¸°ë°˜ ì›”ë³„ SHAP ê³„ì‚° ì™„ë£Œ")

# --------------------------------------------------
# 5-1) Nelson-Aalen hazardì™€ SHAP ê¸°ë°˜ ë³€ìˆ˜ ê¸°ì—¬ë„ì˜ ìƒê´€ì„± ë¶„ì„
# --------------------------------------------------

from scipy.stats import pearsonr

# ì „ì²´ SHAP ì¤‘ìš”ë„ ê¸°ì¤€ ìƒìœ„ 3ê°œ ë³€ìˆ˜ ì„ íƒ
global_mean_abs = shap_df[features].abs().mean()
top3_features = global_mean_abs.sort_values(ascending=False).head(3).index.tolist()

# í•´ë‹¹ ë³€ìˆ˜ë“¤ì˜ ì›”ë³„ í‰ê· ê°’ í•©ê³„ (monthly_shap ê¸°ì¤€)
monthly_shap_top3_sum = monthly_shap[top3_features].sum(axis=1)

# ê³µí†µ ì›”ë§Œ ì‚¬ìš©í•˜ì—¬ monthly_hazardsì™€ ì •ë ¬
common_months = monthly_shap_top3_sum.index.intersection(pd.PeriodIndex(monthly_hazards.keys(), freq="M"))
hazard_series = pd.Series(monthly_hazards).astype(float)
hazard_series.index = pd.PeriodIndex(hazard_series.index, freq="M")

aligned_hazard = hazard_series[common_months]
aligned_shap = monthly_shap_top3_sum[common_months]

# ìƒê´€ê³„ìˆ˜ ê³„ì‚°
r, p = pearsonr(aligned_hazard.values, aligned_shap.values)
print(f"ğŸ“Š Nelson-Aalen hazardì™€ SHAP Top-3 í•©ê³„ì˜ Pearson ìƒê´€ê³„ìˆ˜: r = {r:.3f}, p = {p:.3f}")

# --------------------------------------------------
# 6) í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
# --------------------------------------------------

# âœ… 6-1. Concordance Index ê³„ì‚°
from lifelines.utils import concordance_index

c_index = concordance_index(y_test["T"], -model.predict(xgb.DMatrix(X_test)), y_test["E"])
print(f"Concordance Index (C-index): {c_index:.4f}")

# âœ… 6-2. Integrated Brier Score ê³„ì‚° (scikit-survival í•„ìš”)
from sksurv.metrics import integrated_brier_score
from sksurv.util import Surv

# (1) scikit-survival í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
y_train_sksurv = Surv.from_arrays(event=y_train["E"].astype(bool), time=y_train["T"])
y_test_sksurv  = Surv.from_arrays(event=y_test["E"].astype(bool), time=y_test["T"])

# (2) AFT ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ì‚¬ìš©
predicted = model.predict(xgb.DMatrix(X_test))

 # (3) IBS ê³„ì‚° (ì˜ˆ: í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë‚´ ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ì‹œì  ì„¤ì •)
t_min = y_test["T"].min()
t_max = y_test["T"].max()
times = np.linspace(t_min, t_max * 0.999, 50)
ibs = integrated_brier_score(y_train_sksurv, y_test_sksurv, predicted, times)
print(f"Integrated Brier Score (IBS): {ibs:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns

# ì‹œê°í™”: ê° ê·¸ë£¹ì—ì„œ top featureë“¤ì˜ SHAP ì´í•© ë¹„ì¤‘ (ë¹„ìœ¨ ê¸°ë°˜ ì¤‘ìš”ë„)
monthly_df = monthly_shap
mean_abs = monthly_df.abs().sum()
mean_abs = mean_abs / mean_abs.sum()  # Normalize to sum=1
top10 = mean_abs.sort_values(ascending=False).head(10)

# SHAP ì¤‘ìš”ë„ ë°” í”Œë¡¯
plt.figure(figsize=(6, 4))
sns.barplot(x=top10.values, y=top10.index)
plt.title(f"Top 10 features SHAP importance")
plt.xlabel("SHAP importance")
plt.ylabel("feature")
plt.tight_layout()
plt.show()

# ì‹œê°í™”: Top 10 featureë“¤ì˜ ì›”ë³„ SHAP í‰ê· ê°’ ì¶”ì´
plt.figure(figsize=(12, 6))
for col in top10.index:
    sns.lineplot(x=monthly_df.index.astype(str), y=monthly_df[col], label=col)
plt.title(f"Monthly SHAP average for Top 10 features")
plt.xlabel("issue_month")
plt.ylabel("Average SHAP value")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()