import numpy as np
import pandas as pd
from lifelines import NelsonAalenFitter
import warnings
import shap

warnings.filterwarnings("ignore", category=UserWarning)

DATA_PATH = "../../data/processed/lendingclub_features_for_rf.parquet"
FEATURE_PATH = "../../data/processed/features_final_list_rf.csv"

# --------------------------------------------------
# 1) ë°ì´í„° ë¡œë“œ & í•„í„°
# --------------------------------------------------
df = pd.read_parquet(DATA_PATH)
df["issue_d"] = pd.to_datetime(df["issue_d"], errors="coerce")
df["last_pymnt_d"] = pd.to_datetime(df["last_pymnt_d"], errors="coerce")

# ê²°ì¸¡í•­ ì œê±°  
print(f"issue_d NaT ê°œìˆ˜: {df['issue_d'].isna().sum()}")
print(f"last_pymnt_d NaT ê°œìˆ˜: {df['last_pymnt_d'].isna().sum()}")
df = df[df['issue_d'].notna() & df['last_pymnt_d'].notna()]  # NaT ì œê±°


# --------------------------------------------------
# COVIDâ€‘19 ë…¸ì¶œ ë³€ìˆ˜ ì •ì˜
#   - cutâ€‘off ë‚ ì§œ : 2020â€‘03â€‘01
#   - ë¹„â€‘ë¶€ë„(Charged Off/Default ì•„ë‹˜) ëŒ€ì¶œ  â†’  issue_d + term â‰¤ cutâ€‘off
#   - ë¶€ë„ ëŒ€ì¶œ                              â†’  last_pymnt_d â‰¤ cutâ€‘off
# --------------------------------------------------
cutoff = pd.to_datetime("2020-03-01")

# termì´ '36 months' ê°™ì€ ë¬¸ìì—´ì´ë©´ ìˆ«ì(ì›”ìˆ˜)ë§Œ ì¶”ì¶œ
df["term"] = (
    df["term"].astype(str)
      .str.extract(r"(\d+)")[0]
      .astype(int)
)

# ë§Œê¸°ì¼ = issue_d + term_m Ã— 30ì¼ (ì›” ë‹¨ìœ„ ê·¼ì‚¬)
df["maturity_d"] = df["issue_d"] + pd.to_timedelta(df["term"] * 30, unit="D")

# ë¶€ë„(event==1)  â†’  ë§ˆì§€ë§‰ ìƒí™˜ì¼ì´ cutoff **ì´ì „(í¬í•¨)** ì´ë©´ ì½”ë¡œë‚˜ ë…¸ì¶œ
# ê·¸ ì™¸            â†’  ë§Œê¸°ì¼ì´ cutoff **ì´ì „(í¬í•¨)** ì´ë©´ ì½”ë¡œë‚˜ ë…¸ì¶œ
df["covid_exposure"] = np.where(
    df["E"] == 1,
    df["last_pymnt_d"] >= cutoff,
    df["maturity_d"]   >= cutoff
).astype(int)

# Calculate survival duration:
# If event occurred (event == 1), use duration until event (e.g. charged off)
# Else, use duration until end of observation (censored)
# T / event ê²°ì¸¡ & ìŒìˆ˜ ì œê±°
df = df.dropna(subset=["T", "E"])
df = df[df["T"] >= 0]

# --------------------------------------------------
# 2) íŠ¹ì§• í–‰ë ¬ / íƒ€ê¹ƒ
# --------------------------------------------------
features = pd.read_csv(FEATURE_PATH)["feature"].tolist()
features = [f for f in features if f in df.columns]
features = [f for f in features if f not in ["T", "E"] and str(df[f].dtype) not in ["object", "datetime64[ns]"]]

# issue_month ì¸ì½”ë”©: XGBoost í•™ìŠµì„ ìœ„í•œ ì‹œê°„ ë³€ìˆ˜ ì¶”ê°€
df["issue_month"] = df["issue_d"].dt.to_period("M")
df["issue_month_encoded"] = df["issue_month"].astype("category").cat.codes

## features.append("issue_month_encoded")  # ì œê±°: ì‹¤í—˜ ëª©ì ìƒ issue_month_encodedë¥¼ featureì—ì„œ ì œì™¸

X = df[features]


# â–¼â–¼â–¼ ì¶”ê°€ ì½”ë“œ â–¼â–¼â–¼  
print("â–¶ ë°ì´í„° ë¶„í¬ í™•ì¸:")
print(f"- ì „ì²´ ë°ì´í„°: {len(df)}")
print(f"- COVID ë…¸ì¶œ ê·¸ë£¹: {df['covid_exposure'].sum()}")
print(f"- issue_d < cutoff: {(df['issue_d'] < cutoff).sum()}")
print(f"- ìµœì†Œ issue_d: {df['issue_d'].min()}, ìµœëŒ€ issue_d: {df['issue_d'].max()}")

df = df[df['covid_exposure'] == 1]
X = df[features].copy()

# --------------------------------------------------
# 3) ì‹œê°„ìˆœ ë¶„í• 
# --------------------------------------------------
# í›ˆë ¨ ë°ì´í„° ì¸ë±ìŠ¤ ë§ˆìŠ¤í¬: issue_d < cutoff & covid_exposure == 0
train_mask = (df['issue_d'] < cutoff).reindex(X.index).fillna(False)

# Debug: inspect mask components
print(f"â–¶ Total rows: {len(df)}")
print(f"â–¶ Rows with issue_d < cutoff ({cutoff.date()}): {int((df['issue_d'] < cutoff).sum())}")

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test = X.loc[train_mask], X.loc[~train_mask]
y_train = df.loc[train_mask, ["T", "E"]]
y_test  = df.loc[~train_mask, ["T", "E"]]
print(X_train.dtypes.value_counts()) # category íƒ€ì…ê³¼ ìˆ«ìí˜•(int/float)ë§Œ ì¡´ì¬í•´ì•¼ í•¨


# Fallback if no training data with covid_exposure == 0
if X_train.shape[0] == 0:
    print("âš ï¸ No non-exposed training data; using time-based split only.")
    train_mask = df["issue_d"] < cutoff
    X_train, X_test = X.loc[train_mask], X.loc[~train_mask]
    y_train = df.loc[train_mask, ["T", "E"]]
    y_test  = df.loc[~train_mask, ["T", "E"]]


# --------------------------------------------------
# 4) ê´€ì¸¡ ì‹œì  ê¸°ì¤€ ëˆ„ì  Nelson-Aalen Hazard ì¶”ì •
# --------------------------------------------------
naf = NelsonAalenFitter()
df["obs_month"] = (df["issue_d"] + pd.to_timedelta(df["T"], unit="D")).dt.to_period("M")
observation_months = sorted(df["obs_month"].unique())

na_obs_hazard = {}
na_obs_events = {}

for obs_month in observation_months:
    df_window = df[df["obs_month"] <= obs_month]
    if df_window.shape[0] < 100:
        continue
    try:
        naf.fit(df_window["T"], event_observed=df_window["E"])
        na_obs_hazard[str(obs_month)] = float(naf.cumulative_hazard_.iloc[-1, 0])
        na_obs_events[str(obs_month)] = int(df_window["E"].sum())
    except Exception as e:
        print(f"âš ï¸ {obs_month} ê´€ì¸¡ê¸°ì¤€ NA hazard ê³„ì‚° ì˜¤ë¥˜: {e}")
        continue

# ì‹œê°í™”
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.plot(na_obs_hazard.keys(), na_obs_hazard.values(), marker="o", label="NA cumulative hazard by obs_month")
plt.title("Cumulative Hazard by Observation Month (Nelson-Aalen)")
plt.xlabel("observation month")
plt.ylabel("Cumulative Hazard")
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
print("âœ… ê´€ì¸¡ ê¸°ì¤€ NA ëˆ„ì  hazard ì¶”ì • ì™„ë£Œ")

# --------------------------------------------------
# 4-c) ê´€ì¸¡ ì‹œì  ê¸°ì¤€ NA ëˆ„ì ìœ„í—˜ ì›”ë³„ ë³€í™”ëŸ‰ ê³„ì‚°
# --------------------------------------------------
na_obs_delta = {}
na_obs_sorted_keys = sorted(na_obs_hazard.keys())

for i in range(1, len(na_obs_sorted_keys)):
    prev_month = na_obs_sorted_keys[i - 1]
    curr_month = na_obs_sorted_keys[i]
    delta = na_obs_hazard[curr_month] - na_obs_hazard[prev_month]
    na_obs_delta[curr_month] = delta

# ê²°ê³¼ ì €ì¥
na_obs_delta_df = pd.DataFrame({
    "month": list(na_obs_delta.keys()),
    "delta": list(na_obs_delta.values())
})
na_obs_delta_df.to_csv("../../reports/monthly_na_delta.csv", index=False)
print("ğŸ“ ê´€ì¸¡ ê¸°ì¤€ NA ì›”ë³„ ìœ„í—˜ ì¦ê°€ëŸ‰ ì €ì¥ ì™„ë£Œ: monthly_na_delta.csv")

# --------------------------------------------------
# 4-d) ê´€ì¸¡ ê¸°ì¤€ NA ì›”ë³„ ìœ„í—˜ ì¦ê°€ëŸ‰ ì‹œê°í™”
# --------------------------------------------------
plt.figure(figsize=(12, 5))
plt.plot(na_obs_delta.keys(), na_obs_delta.values(), marker="o", label="Î” NA hazard (month-over-month)")
plt.title("Monthly Increase in NA Cumulative Hazard (Observation Month)")
plt.xlabel("Observation Month")
plt.ylabel("Hazard Increase (Î”)")
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
print("ğŸ“Š ê´€ì¸¡ ê¸°ì¤€ NA ëˆ„ì ìœ„í—˜ ì¦ê°€ëŸ‰ ì‹œê°í™” ì™„ë£Œ")

# --------------------------------------------------
# 5-2) ì›”ë³„ ëˆ„ì  SHAP ë¶„ì„ ë° ìœ„í—˜ ì¶”ì • ì €ì¥
# --------------------------------------------------

window_results = {}
shap_top_records = []
monthly_shap_dynamic = []
monthly_shap_dynamic_std = []
month_labels = []

unique_months = sorted(df["issue_month"].unique())

for month in unique_months:
    # ëˆ„ì  í•™ìŠµ ë°ì´í„° ìƒì„±
    df_window = df[df["issue_month"] <= month]
    if df_window.shape[0] < 300:
        continue
    if df_window.shape[0] > 200000:
        df_window = df_window.sample(n=20000, random_state=42)
    # íŠ¹ì§• í–‰ë ¬ ë° íƒ€ê¹ƒ êµ¬ì„±
    X_window = df_window[features]
    y_window = df_window[["T", "E"]]
    y_lower = np.where(y_window["E"] == 1, y_window["T"], -np.inf)
    y_upper = y_window["T"]

    # DMatrix êµ¬ì„±
    import xgboost as xgb
    dtrain_window = xgb.DMatrix(data=X_window, label=y_upper)
    dtrain_window.set_float_info("label_lower_bound", y_lower)
    dtrain_window.set_float_info("label_upper_bound", y_upper)

    params = {
        "objective": "survival:aft",
        "aft_loss_distribution": "logistic",
        "aft_loss_distribution_scale": 1.0,
        "learning_rate": 0.05,
        "max_depth": 4,
        "subsample": 0.8,
        "colsample_bynode": 0.8,
        "random_state": 42,
        "nthread": -1,
        "verbosity": 1
    }

    # ëª¨ë¸ í•™ìŠµ
    model_window = xgb.train(
        params=params,
        dtrain=dtrain_window,
        num_boost_round=100
    )

    # SHAP ê³„ì‚° (í•™ìŠµì— ì‚¬ìš©ëœ ëˆ„ì  ë°ì´í„° ì „ì²´ì— ëŒ€í•´)
    shap_input = X_window.sample(n=min(20000, len(X_window)), random_state=42)
    import shap
    explainer_window = shap.TreeExplainer(model_window, data=shap_input, feature_perturbation="interventional", approximate=True)
    shap_values_window = explainer_window.shap_values(shap_input)
    shap_df_window = pd.DataFrame(shap_values_window, columns=features)
    monthly_shap_dynamic.append(shap_df_window.mean())
    monthly_shap_dynamic_std.append(shap_df_window.std())
    month_labels.append(str(month))

    # ì›”ë³„ SHAP ìƒìœ„ 10ê°œ ì €ì¥ (long-form)
    top_features = shap_df_window.abs().mean().sort_values(ascending=False).head(10)
    shap_std_window = shap_df_window.std()
    for feature, value in top_features.items():
        shap_top_records.append({
            "issue_month": str(month),
            "feature": feature,
            "mean_abs_shap": value,
            "shap_std": shap_std_window[feature]
        })

# ê²°ê³¼ ì €ì¥
valid_len = min(len(monthly_shap_dynamic), len(month_labels))
monthly_shap_df = pd.DataFrame(monthly_shap_dynamic[:valid_len], index=month_labels[:valid_len])
monthly_shap_df.index.name = "issue_month"
monthly_shap_df.to_csv("../../reports/monthly_shap_dynamic.csv")
print("ğŸ“ ì›”ë³„ ëˆ„ì  SHAP í‰ê· ê°’ ì €ì¥ ì™„ë£Œ: monthly_shap_dynamic.csv")

valid_len_std = min(len(monthly_shap_dynamic_std), len(month_labels))
monthly_shap_std_df = pd.DataFrame(monthly_shap_dynamic_std[:valid_len_std], index=month_labels[:valid_len_std])
monthly_shap_std_df.index.name = "issue_month"
monthly_shap_std_df.to_csv("../../reports/monthly_shap_dynamic_std.csv")
print("ğŸ“ ì›”ë³„ ëˆ„ì  SHAP í‘œì¤€í¸ì°¨ ì €ì¥ ì™„ë£Œ: monthly_shap_dynamic_std.csv")

shap_top_df = pd.DataFrame(shap_top_records)
shap_top_df.to_csv("../../reports/monthly_top10_shap_longform.csv", index=False)
print("ğŸ“ ì›”ë³„ SHAP ìƒìœ„ 10ê°œ ë³€ìˆ˜ ì €ì¥ ì™„ë£Œ: monthly_top10_shap_longform.csv")

# --------------------------------------------------
# 5-1) Nelson-Aalen hazardì™€ SHAP ê¸°ë°˜ ë³€ìˆ˜ ê¸°ì—¬ë„ì˜ ìƒê´€ì„± ë¶„ì„
# --------------------------------------------------

from scipy.stats import pearsonr

# ì „ì²´ SHAP ì¤‘ìš”ë„ ê¸°ì¤€ ìƒìœ„ 3ê°œ ë³€ìˆ˜ ì„ íƒ
global_mean_abs = monthly_shap_df.abs().mean()
top3_features = global_mean_abs.sort_values(ascending=False).head(3).index.tolist()

# í•´ë‹¹ ë³€ìˆ˜ë“¤ì˜ ì›”ë³„ í‰ê· ê°’ í•©ê³„ (monthly_shap ê¸°ì¤€)
monthly_shap_top3_sum = monthly_shap_df[top3_features].sum(axis=1)

# ê³µí†µ ì›”ë§Œ ì‚¬ìš©í•˜ì—¬ na_obs_hazardì™€ ì •ë ¬
common_months = monthly_shap_top3_sum.index.intersection(pd.PeriodIndex(na_obs_hazard.keys(), freq="M"))
hazard_series = pd.Series(na_obs_hazard).astype(float)
hazard_series.index = pd.PeriodIndex(hazard_series.index, freq="M")

aligned_hazard = hazard_series[common_months]
aligned_shap = monthly_shap_top3_sum[common_months]

# ìƒê´€ê³„ìˆ˜ ê³„ì‚°
if len(aligned_hazard) >= 2 and len(aligned_shap) >= 2:
    r, p = pearsonr(aligned_hazard.values, aligned_shap.values)
    print(f"ğŸ“Š Nelson-Aalen hazardì™€ SHAP Top-3 í•©ê³„ì˜ Pearson ìƒê´€ê³„ìˆ˜: r = {r:.3f}, p = {p:.3f}")
else:
    print("âš ï¸ ìƒê´€ê³„ìˆ˜ ê³„ì‚° ë¶ˆê°€: ê³µí†µ ì›” ê°œìˆ˜ê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤.")

# --------------------------------------------------
# 6) ì›”ë³„ ëˆ„ì  ëª¨ë¸ ê¸°ë°˜ C-index ë° IBS ì €ì¥
# --------------------------------------------------

from lifelines.utils import concordance_index
from sksurv.metrics import integrated_brier_score
from sksurv.util import Surv
from scipy.stats import norm
import xgboost as xgb
import numpy as np
import pandas as pd

cindex_records = []
ibs_records = []

for i, month in enumerate(month_labels):
    df_window = df[df["issue_month"] <= month]
    if df_window.shape[0] < 300:
        continue

    X_window = df_window[features]
    y_window = df_window[["T", "E"]]

    X_window = X_window.copy()
    y_window = y_window.copy()

    # C-index
    cidx = concordance_index(
        y_window["T"], 
        -monthly_shap_dynamic[i].values @ X_window[monthly_shap_dynamic[i].index].T.values,  # linear SHAP proxy score
        y_window["E"]
    )
    cindex_records.append({"issue_month": month, "c_index": cidx})

    # IBS (ìƒì¡´í™•ë¥  ê¸°ë°˜)
    y_sksurv = Surv.from_arrays(event=y_window["E"].astype(bool), time=y_window["T"])
    # Use the model from last training iteration for prediction
    # For this, we need to re-train or store model_window from above; assuming model_window is last trained model
    # But model_window is overwritten in the loop, so to use correct model, we can re-train or skip
    # For simplicity, use model_window from last iteration (month_labels[-1])
    # So we re-train here for each month to get model_window
    y_lower = np.where(y_window["E"] == 1, y_window["T"], -np.inf)
    y_upper = y_window["T"]
    dtrain_window = xgb.DMatrix(data=X_window, label=y_upper)
    dtrain_window.set_float_info("label_lower_bound", y_lower)
    dtrain_window.set_float_info("label_upper_bound", y_upper)
    params = {
        "objective": "survival:aft",
        "aft_loss_distribution": "logistic",
        "aft_loss_distribution_scale": 1.0,
        "learning_rate": 0.05,
        "max_depth": 4,
        "subsample": 0.8,
        "colsample_bynode": 0.8,
        "random_state": 42,
        "nthread": -1,
        "verbosity": 1
    }
    model_window = xgb.train(
        params=params,
        dtrain=dtrain_window,
        num_boost_round=100
    )
    pred = model_window.predict(xgb.DMatrix(X_window))
    sigma = params["aft_loss_distribution_scale"]
    t_min = y_window["T"].min()
    t_max = y_window["T"].max()
    times = np.linspace(t_min, t_max * 0.999, 50)

    estimate = np.zeros((len(pred), len(times)))
    for j, t in enumerate(times):
        estimate[:, j] = 1 - norm.cdf(np.log(t), loc=np.log(pred), scale=sigma)

    ibs = integrated_brier_score(y_sksurv, y_sksurv, estimate, times)
    ibs_records.append({"issue_month": month, "ibs": ibs})

# Save results
cindex_df = pd.DataFrame(cindex_records)
cindex_df.to_csv("../../reports/monthly_cindex.csv", index=False)

ibs_df = pd.DataFrame(ibs_records)
ibs_df.to_csv("../../reports/monthly_ibs.csv", index=False)
print("ğŸ“ ì›”ë³„ C-index ë° IBS ì €ì¥ ì™„ë£Œ")